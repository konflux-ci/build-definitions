apiVersion: tekton.dev/v1
kind: Task
metadata:
  annotations:
    tekton.dev/pipelines.minVersion: 0.12.1
    tekton.dev/tags: image-build, konflux
  labels:
    app.kubernetes.io/version: "0.2"
    build.appstudio.redhat.com/build_type: oci-copy
  name: oci-copy
spec:
  description: |-
    Copy content from arbitrary urls into the OCI registry. Downloads and pushes files in parallel
    (PARALLEL_JOBS, default 8). Requires disk space for concurrent downloads (e.g., 8 x 10GB = 80GB).
  params:
    - description: Reference of the image we will push
      name: IMAGE
      type: string
    - default: ./oci-copy.yaml
      description: Path to the oci copy file.
      name: OCI_COPY_FILE
      type: string
    - name: BEARER_TOKEN_SECRET_NAME
      description: >-
        Name of a secret which will be made available to the build as an Authorization header. Note, the token will
        be sent to all servers found in the oci-copy.yaml file. If you do not wish to send the token to all servers,
        different taskruns and therefore different oci artifacts must be used.
      type: string
      default: "does-not-exist"
    - name: AWS_SECRET_NAME
      description: >-
        Name of a secret which will be made available to the build for downloading from Amazon S3 or S3-compatible
        storage using AWS CLI with parallel multipart transfers. If specified, this will take precedence over
        BEARER_TOKEN_SECRET_NAME. The secret must contain two keys: `aws_access_key_id` and `aws_secret_access_key`.
      type: string
      default: "does-not-exist"
    - name: SBOM_TYPE
      description: "Select the SBOM format to generate. Valid values: spdx, cyclonedx."
      type: string
      default: spdx
    - name: PARALLEL_JOBS
      description: "Number of files to download/push in parallel."
      type: string
      default: "8"
  results:
    - description: Digest of the artifact just pushed
      name: IMAGE_DIGEST
    - description: Repository where the artifact was pushed
      name: IMAGE_URL
    - description: Link to the SBOM blob pushed to the registry.
      name: SBOM_BLOB_URL
    - name: IMAGE_REF
      description: Image reference of the built image
  stepTemplate:
    env:
      - name: OCI_COPY_FILE
        value: $(params.OCI_COPY_FILE)
      - name: IMAGE
        value: $(params.IMAGE)
      - name: SBOM_TYPE
        value: $(params.SBOM_TYPE)
      - name: PARALLEL_JOBS
        value: $(params.PARALLEL_JOBS)
  steps:
    - name: prepare
      image: quay.io/konflux-ci/yq:latest@sha256:eb5b7311cff72c42f1d816fcf4f565829f58bbbd4c51503ed390b8f4e7a3fe89
      script: |
        #!/bin/bash
        set -eu
        set -o pipefail

        oci_copy_file_path="$(pwd)/source/$OCI_COPY_FILE"

        mkdir -p "$(workspaces.source.path)/vars/"

        for entry in $(cat $oci_copy_file_path | yq '.artifacts[] | @json | @base64'); do
          entry=$(echo $entry | base64 -d)
          source=$(echo $entry | yq .source)
          filename=$(echo $entry | yq .filename)
          artifact_type=$(echo $entry | yq .type)
          artifact_digest=$(echo $entry | yq .sha256sum)
          varfile="${filename//\//-}"

          {
            echo "declare OCI_SOURCE=${source}";
            echo "declare OCI_FILENAME=${filename}";
            echo "declare OCI_ARTIFACT_TYPE=${artifact_type}";
            echo "declare OCI_ARTIFACT_DIGEST=${artifact_digest}";
          } > "$(workspaces.source.path)/vars/$varfile"

          echo "Wrote $(workspaces.source.path)/vars/$varfile with contents:"
          cat "$(workspaces.source.path)/vars/$varfile"
        done
      workingDir: $(workspaces.source.path)
    - name: oci-copy
      image: quay.io/konflux-ci/task-runner:1.1.1@sha256:c3b31a65d6e82a307f0f2f6eb5762548e655695949fd42735ec8f178142d6f14
      computeResources:
        limits:
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        capabilities:
          add:
            - SETFCAP
      env:
      - name: BEARER_TOKEN
        valueFrom:
          secretKeyRef:
            name: $(params.BEARER_TOKEN_SECRET_NAME)
            key: token
            optional: true
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: $(params.AWS_SECRET_NAME)
            key: aws_access_key_id
            optional: true
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: $(params.AWS_SECRET_NAME)
            key: aws_secret_access_key
            optional: true
      script: |
        #!/bin/bash
        set -euo pipefail

        # S3 transfer settings optimized for parallel multipart downloads
        MAX_CONCURRENT=50
        MULTIPART_THRESHOLD="64MB"
        MULTIPART_CHUNKSIZE="16MB"

        # Use NVMe-backed emptyDir for downloads (faster than workspace PVC)
        DOWNLOAD_DIR="/var/lib/containers/downloads"
        mkdir -p "$DOWNLOAD_DIR"

        # Configure AWS CLI once (avoid race condition in parallel downloads)
        mkdir -p ~/.aws
        printf '%s\n' \
          "[default]" \
          "s3 =" \
          "  max_concurrent_requests = ${MAX_CONCURRENT}" \
          "  multipart_threshold = ${MULTIPART_THRESHOLD}" \
          "  multipart_chunksize = ${MULTIPART_CHUNKSIZE}" \
          > ~/.aws/config

        download() {
          local url="$1"
          local file="$2"

          local directory
          directory=$(dirname "${file}")
          echo "Found directory of ${file} to be ${directory}"
          if [ ! -d "${directory}" ]; then
            echo "Found that directory ${directory} does not exist. Creating."
            mkdir -p "${directory}"
          fi

          if [ -n "${AWS_ACCESS_KEY_ID:-}" ] && [ -n "${AWS_SECRET_ACCESS_KEY:-}" ]; then
            echo "Found both aws credentials secret with both aws_access_key_id and aws_secret_access_key. Assuming S3 bucket"

            # Extract host and path from URL
            local host path
            host=$(echo -n "$url" | awk -F '/' '{print $3}')
            path=$(echo "$url" | cut -d/ -f4-)

            echo "Parsing S3 URL - Host: ${host}, Path: ${path}"

            local endpoint_url="" bucket="" region="" s3_uri="" key=""

            if [[ "$host" == *.s3.*.amazonaws.com ]]; then
              # AWS virtual-hosted-style: bucket.s3.region.amazonaws.com/key
              bucket=$(echo "$host" | cut -d. -f1)
              region=$(echo "$host" | cut -d. -f3)
              s3_uri="s3://${bucket}/${path}"
              echo "Detected AWS virtual-hosted style URL"
            elif [[ "$host" == s3.*.amazonaws.com ]]; then
              # AWS path-style: s3.region.amazonaws.com/bucket/key
              region=$(echo "$host" | cut -d. -f2)
              bucket=$(echo "$path" | cut -d/ -f1)
              key=$(echo "$path" | cut -d/ -f2-)
              s3_uri="s3://${bucket}/${key}"
              echo "Detected AWS path-style URL"
            elif [[ "$host" == *.s3.*.cloud-object-storage.appdomain.cloud ]]; then
              # IBM Cloud COS virtual-hosted: bucket.s3.region.cloud-object-storage.appdomain.cloud/key
              bucket=$(echo "$host" | cut -d. -f1)
              region=$(echo "$host" | cut -d. -f3)
              s3_uri="s3://${bucket}/${path}"
              endpoint_url="https://s3.${region}.cloud-object-storage.appdomain.cloud"
              echo "Detected IBM Cloud COS virtual-hosted style URL"
            elif [[ "$host" == s3.*.cloud-object-storage.appdomain.cloud ]]; then
              # IBM Cloud COS path-style: s3.region.cloud-object-storage.appdomain.cloud/bucket/key
              region=$(echo "$host" | cut -d. -f2)
              bucket=$(echo "$path" | cut -d/ -f1)
              key=$(echo "$path" | cut -d/ -f2-)
              s3_uri="s3://${bucket}/${key}"
              endpoint_url="https://${host}"
              echo "Detected IBM Cloud COS path-style URL"
            elif [[ "$host" == *s3* ]] || [[ "$host" == *minio* ]] || [[ "$host" == *storage* ]]; then
              # Generic S3-compatible storage: host/bucket/key
              bucket=$(echo "$path" | cut -d/ -f1)
              key=$(echo "$path" | cut -d/ -f2-)
              s3_uri="s3://${bucket}/${key}"
              endpoint_url="https://${host}"
              echo "Detected generic S3-compatible storage URL"
            else
              # URL doesn't look like S3, fall back to curl
              echo "URL doesn't match known S3 patterns, falling back to curl"
              curl --fail --silent --show-error --location "$url" -o "$file"
              return
            fi

            echo "S3 URI: ${s3_uri}, Bucket: ${bucket}, Region: ${region:-auto}"
            [ -n "${endpoint_url}" ] && echo "Endpoint URL: ${endpoint_url}"

            if ! command -v aws &> /dev/null; then
              echo "ERROR: AWS CLI is required for S3 downloads but was not found"
              exit 1
            fi

            echo "Using AWS CLI for download with parallel multipart transfers"
            if [ -n "${endpoint_url}" ]; then
              aws s3 cp --only-show-errors --endpoint-url "${endpoint_url}" "${s3_uri}" "${file}"
            else
              aws s3 cp --only-show-errors "${s3_uri}" "${file}"
            fi
          elif [ -n "${BEARER_TOKEN:-}" ]; then
            echo "Found bearer token. Using it for authentication."
            curl --fail --silent --show-error -H "Authorization: Bearer ${BEARER_TOKEN}" --location "$url" -o "$file"
          else
            curl --fail --silent --show-error --location "$url" -o "$file"
          fi
        }

        set -u

        echo "Selecting auth for $IMAGE"
        select-oci-auth $IMAGE > auth.json

        echo "Extracting artifact_type"
        ARTIFACT_TYPE=$(cat "$(pwd)/source/$OCI_COPY_FILE" | yq '.artifact_type')

        REPO=${IMAGE%:*}
        echo "Found that ${REPO} is the repository for ${IMAGE}"

        cat >artifact-manifest.json <<EOL
        {
          "schemaVersion": 2,
          "mediaType": "application/vnd.oci.image.manifest.v1+json",
          "artifactType": "${ARTIFACT_TYPE}",
          "config": {
            "mediaType": "application/vnd.oci.empty.v1+json",
            "digest": "sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a",
            "size": 2,
            "data": "e30="
          },
          "layers": [],
          "annotations": {
            "org.opencontainers.image.created": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
        }
        EOL

        echo "Ensuring that the empty blob exists, for the image manifest config."
        if ! echo -n "{}" | retry oras blob push \
                --registry-config auth.json \
                ${REPO}@sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a \
                --media-type application/vnd.oci.empty.v1+json --size 2 -
        then
          echo "Failed to push empty blob to registry"
          exit 1
        fi

        # Validate PARALLEL_JOBS
        if ! [[ "$PARALLEL_JOBS" =~ ^[1-9][0-9]*$ ]]; then
          echo "ERROR: PARALLEL_JOBS must be a positive integer, got: $PARALLEL_JOBS"
          exit 1
        fi

        # Collect all varfiles
        shopt -s nullglob
        ALL_VARFILES=("$(workspaces.source.path)"/vars/*)
        shopt -u nullglob
        TOTAL=${#ALL_VARFILES[@]}

        if [ "$TOTAL" -eq 0 ]; then
          echo "No artifacts to process"
        else
          echo "Processing $TOTAL files in batches of $PARALLEL_JOBS"
        fi

        for ((batch=0; batch<TOTAL; batch+=PARALLEL_JOBS)); do
          BATCH_END=$((batch + PARALLEL_JOBS))
          [ "$BATCH_END" -gt "$TOTAL" ] && BATCH_END=$TOTAL

          echo "=== Batch $((batch/PARALLEL_JOBS + 1)): files $batch-$((BATCH_END-1)) ==="

          # Check which files need processing (parallel existence checks)
          declare -a TO_PROCESS=()
          declare -a CHECK_RESULTS=()
          PIDS=()
          for ((i=batch; i<BATCH_END; i++)); do
            (
              # shellcheck source=/dev/null
              source "${ALL_VARFILES[$i]}"
              if oras blob fetch --registry-config auth.json --descriptor "${REPO}@sha256:${OCI_ARTIFACT_DIGEST}" &>/dev/null; then
                exit 0  # exists
              else
                exit 1  # missing
              fi
            ) &
            PIDS+=("$!")
            CHECK_RESULTS+=("$i")
          done

          for idx in "${!PIDS[@]}"; do
            i="${CHECK_RESULTS[$idx]}"
            # shellcheck source=/dev/null
            source "${ALL_VARFILES[$i]}"
            if wait "${PIDS[$idx]}"; then
              echo "Blob ${OCI_FILENAME} exists, skipping"
            else
              TO_PROCESS+=("${ALL_VARFILES[$i]}")
            fi
          done

          # Phase 1: Parallel downloads
          if [ ${#TO_PROCESS[@]} -gt 0 ]; then
            echo "--- Downloading ${#TO_PROCESS[@]} files ---"
            declare -A PID_TO_FILE=()
            for varfile in "${TO_PROCESS[@]}"; do
              (
                # shellcheck source=/dev/null
                source "$varfile"
              DLPATH="${DOWNLOAD_DIR}/${OCI_FILENAME}"
              echo "[DL] Downloading ${OCI_FILENAME}..."
              if ! download "$OCI_SOURCE" "$DLPATH"; then
                echo "[DL] ERROR: Failed to download ${OCI_FILENAME}"
                exit 1
              fi
              if ! echo "$OCI_ARTIFACT_DIGEST  $DLPATH" | sha256sum --check --quiet; then
                echo "[DL] ERROR: Checksum mismatch for ${OCI_FILENAME}"
                exit 1
              fi
              echo "[DL] Downloaded ${OCI_FILENAME} [OK]"
            ) &
              PID_TO_FILE[$!]="$varfile"
            done
            FAILED=0
            for pid in "${!PID_TO_FILE[@]}"; do
              if ! wait "$pid"; then
                # shellcheck source=/dev/null
                source "${PID_TO_FILE[$pid]}"
                echo "ERROR: Download failed for ${OCI_FILENAME}"
                FAILED=1
              fi
            done
            [ $FAILED -eq 1 ] && exit 1

            # Phase 2: Parallel pushes
            echo "--- Pushing ${#TO_PROCESS[@]} files ---"
            PID_TO_FILE=()
            for varfile in "${TO_PROCESS[@]}"; do
              (
                # shellcheck source=/dev/null
                source "$varfile"
                DLPATH="${DOWNLOAD_DIR}/${OCI_FILENAME}"
                echo "[PUSH] Pushing ${OCI_FILENAME}..."
                if ! retry oras blob push --registry-config auth.json "${REPO}" --media-type "${OCI_ARTIFACT_TYPE}" "$DLPATH"; then
                  echo "[PUSH] ERROR: Failed to push ${OCI_FILENAME}"
                  exit 1
                fi
                echo "[PUSH] Pushed ${OCI_FILENAME} [OK]"
              ) &
              PID_TO_FILE[$!]="$varfile"
            done
            FAILED=0
            for pid in "${!PID_TO_FILE[@]}"; do
              if ! wait "$pid"; then
                # shellcheck source=/dev/null
                source "${PID_TO_FILE[$pid]}"
                echo "ERROR: Push failed for ${OCI_FILENAME}"
                FAILED=1
              fi
            done
            [ $FAILED -eq 1 ] && exit 1

            # Cleanup
            for varfile in "${TO_PROCESS[@]}"; do
              # shellcheck source=/dev/null
              source "$varfile"
              rm -f "${DOWNLOAD_DIR}/${OCI_FILENAME}"
            done
          fi

          # Fetch descriptors in parallel
          echo "--- Fetching descriptors ---"
          declare -A PID_TO_FILE=()
          for ((i=batch; i<BATCH_END; i++)); do
            (
              # shellcheck source=/dev/null
              source "${ALL_VARFILES[$i]}"
              echo "[DESC] Fetching descriptor for ${OCI_FILENAME}..."
              if ! retry oras blob fetch --registry-config auth.json --descriptor "${REPO}@sha256:${OCI_ARTIFACT_DIGEST}" > "${DOWNLOAD_DIR}/desc_${i}.json" 2>&1; then
                echo "[DESC] ERROR: Failed to fetch descriptor for ${OCI_FILENAME}"
                cat "${DOWNLOAD_DIR}/desc_${i}.json" 2>/dev/null || true
                exit 1
              fi
              OCI_ARTIFACT_TYPE="$OCI_ARTIFACT_TYPE" yq -oj -i '.mediaType = env(OCI_ARTIFACT_TYPE)' "${DOWNLOAD_DIR}/desc_${i}.json"
              OCI_FILENAME="$OCI_FILENAME" yq -oj -i '.annotations."org.opencontainers.image.title" = env(OCI_FILENAME)' "${DOWNLOAD_DIR}/desc_${i}.json"
              echo "[DESC] Fetched descriptor for ${OCI_FILENAME} [OK]"
            ) &
            PID_TO_FILE[$!]="${ALL_VARFILES[$i]}"
          done
          FAILED=0
          for pid in "${!PID_TO_FILE[@]}"; do
            if ! wait "$pid"; then
              # shellcheck source=/dev/null
              source "${PID_TO_FILE[$pid]}"
              echo "ERROR: Descriptor fetch failed for ${OCI_FILENAME}"
              FAILED=1
            fi
          done
          [ $FAILED -eq 1 ] && exit 1

          # Append to manifest (sequential - file writes not parallelizable)
          for ((i=batch; i<BATCH_END; i++)); do
            yq -oj -i ".layers += $(cat "${DOWNLOAD_DIR}/desc_${i}.json")" artifact-manifest.json
            rm -f "${DOWNLOAD_DIR}/desc_${i}.json"
          done

          unset TO_PROCESS PIDS PID_TO_FILE CHECK_RESULTS FAILED
        done

        echo "Pushing complete artifact manifest to ${IMAGE}"
        if ! retry oras manifest push --registry-config auth.json "${IMAGE}" artifact-manifest.json
        then
          echo "Failed to push complete artifact manifest to ${IMAGE}"
          exit 1
        fi

        if ! RESULTING_DIGEST=$(retry oras resolve --registry-config auth.json "${IMAGE}")
        then
          echo "Failed to get digest for ${IMAGE} from registry"
          exit 1
        fi
        echo -n "$RESULTING_DIGEST" | tee "$(results.IMAGE_DIGEST.path)"
        echo -n "$IMAGE" | tee "$(results.IMAGE_URL.path)"
        echo -n "${IMAGE}@${RESULTING_DIGEST}" >"$(results.IMAGE_REF.path)"
      volumeMounts:
        - mountPath: /var/lib/containers
          name: varlibcontainers
      workingDir: $(workspaces.source.path)
    - name: sbom-generate
      image: quay.io/konflux-ci/mobster:1.1.0-1768294847@sha256:ff738d9032c9885731bc96383e720ced2469649fa7148b76b58b7492c9bb82b8
      script: |
        #!/bin/bash
        set -euo pipefail

        IMAGE_URL=$(cat "$(results.IMAGE_URL.path)")
        IMAGE_DIGEST=$(cat "$(results.IMAGE_DIGEST.path)")
        OCI_COPY_FILE_PATH="$(pwd)/source/$OCI_COPY_FILE"

        mobster generate \
          --output sbom.json \
          oci-artifact \
          --oci-copy-yaml "$OCI_COPY_FILE_PATH" \
          --image-pullspec "$IMAGE_URL" \
          --image-digest "$IMAGE_DIGEST" \
          --sbom-type "$SBOM_TYPE"

      workingDir: $(workspaces.source.path)
    - name: upload-sbom
      image: quay.io/konflux-ci/appstudio-utils:1610c1fc4cfc9c9053dbefc1146904a4df6659ef@sha256:90ac97b811073cb99a23232c15a08082b586c702b85da6200cf54ef505e3c50c
      workingDir: $(workspaces.source.path)
      script: |
        # Pre-select the correct credentials to work around cosign not supporting the containers-auth.json spec
        mkdir -p /tmp/auth && select-oci-auth "$(cat "$(results.IMAGE_REF.path)")" > /tmp/auth/config.json
        DOCKER_CONFIG=/tmp/auth cosign attach sbom --sbom sbom.json --type "$SBOM_TYPE" "$(cat "$(results.IMAGE_REF.path)")"
    - name: report-sbom-url
      image: quay.io/konflux-ci/yq:latest@sha256:eb5b7311cff72c42f1d816fcf4f565829f58bbbd4c51503ed390b8f4e7a3fe89
      script: |
        #!/bin/bash
        REPO=${IMAGE%:*}
        echo "Found that ${REPO} is the repository for ${IMAGE}"
        SBOM_DIGEST=$(sha256sum sbom.json | awk '{ print $1 }')
        echo "Found that ${SBOM_DIGEST} is the SBOM digest"
        echo -n "${REPO}@sha256:${SBOM_DIGEST}" | tee $(results.SBOM_BLOB_URL.path)
      workingDir: $(workspaces.source.path)
  volumes:
    - emptyDir: {}
      name: varlibcontainers
  workspaces:
    - description: Workspace containing the source artifacts to copy
      name: source
