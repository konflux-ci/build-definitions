---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: oci-copy-oci-ta
  annotations:
    tekton.dev/pipelines.minVersion: 0.12.1
    tekton.dev/tags: image-build, konflux
  labels:
    app.kubernetes.io/version: "0.2"
    build.appstudio.redhat.com/build_type: oci-copy
spec:
  description: |-
    Copy content from arbitrary urls into the OCI registry. Downloads and pushes files in parallel
    (PARALLEL_JOBS, default 8). Requires disk space for concurrent downloads (e.g., 8 x 10GB = 80GB).
  params:
    - name: AWS_SECRET_NAME
      description: 'Name of a secret which will be made available to the build
        for downloading from Amazon S3 or S3-compatible storage using AWS
        CLI with parallel multipart transfers. If specified, this will take
        precedence over BEARER_TOKEN_SECRET_NAME. The secret must contain
        two keys: `aws_access_key_id` and `aws_secret_access_key`.'
      type: string
      default: does-not-exist
    - name: BEARER_TOKEN_SECRET_NAME
      description: Name of a secret which will be made available to the build
        as an Authorization header. Note, the token will be sent to all servers
        found in the oci-copy.yaml file. If you do not wish to send the token
        to all servers, different taskruns and therefore different oci artifacts
        must be used.
      type: string
      default: does-not-exist
    - name: IMAGE
      description: Reference of the image we will push
      type: string
    - name: OCI_COPY_FILE
      description: Path to the oci copy file.
      type: string
      default: ./oci-copy.yaml
    - name: PARALLEL_JOBS
      description: Number of files to download/push in parallel.
      type: string
      default: "8"
    - name: SBOM_TYPE
      description: 'Select the SBOM format to generate. Valid values: spdx,
        cyclonedx.'
      type: string
      default: spdx
    - name: SOURCE_ARTIFACT
      description: The Trusted Artifact URI pointing to the artifact with
        the application source code.
      type: string
  results:
    - name: IMAGE_DIGEST
      description: Digest of the artifact just pushed
    - name: IMAGE_REF
      description: Image reference of the built image
    - name: IMAGE_URL
      description: Repository where the artifact was pushed
    - name: SBOM_BLOB_URL
      description: Link to the SBOM blob pushed to the registry.
  volumes:
    - name: varlibcontainers
      emptyDir: {}
    - name: workdir
      emptyDir: {}
  stepTemplate:
    env:
      - name: IMAGE
        value: $(params.IMAGE)
      - name: OCI_COPY_FILE
        value: $(params.OCI_COPY_FILE)
      - name: PARALLEL_JOBS
        value: $(params.PARALLEL_JOBS)
      - name: SBOM_TYPE
        value: $(params.SBOM_TYPE)
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
  steps:
    - name: use-trusted-artifact
      image: quay.io/konflux-ci/build-trusted-artifacts:latest@sha256:15d7dc86012e41b10d1eb37679ec03ee75c96436224fadd0938a49dc537aa4ad
      args:
        - use
        - $(params.SOURCE_ARTIFACT)=/var/workdir/source
    - name: prepare
      image: quay.io/konflux-ci/yq:latest@sha256:eb5b7311cff72c42f1d816fcf4f565829f58bbbd4c51503ed390b8f4e7a3fe89
      workingDir: /var/workdir
      script: |
        #!/bin/bash
        set -eu
        set -o pipefail

        oci_copy_file_path="$(pwd)/source/$OCI_COPY_FILE"

        mkdir -p "/var/workdir/vars/"

        for entry in $(cat $oci_copy_file_path | yq '.artifacts[] | @json | @base64'); do
          entry=$(echo $entry | base64 -d)
          source=$(echo $entry | yq .source)
          filename=$(echo $entry | yq .filename)
          artifact_type=$(echo $entry | yq .type)
          artifact_digest=$(echo $entry | yq .sha256sum)
          varfile="${filename//\//-}"

          {
            echo "declare OCI_SOURCE=${source}"
            echo "declare OCI_FILENAME=${filename}"
            echo "declare OCI_ARTIFACT_TYPE=${artifact_type}"
            echo "declare OCI_ARTIFACT_DIGEST=${artifact_digest}"
          } >"/var/workdir/vars/$varfile"

          echo "Wrote /var/workdir/vars/$varfile with contents:"
          cat "/var/workdir/vars/$varfile"
        done
    - name: oci-copy
      image: quay.io/konflux-ci/task-runner:1.1.1@sha256:c3b31a65d6e82a307f0f2f6eb5762548e655695949fd42735ec8f178142d6f14
      workingDir: /var/workdir
      volumeMounts:
        - mountPath: /var/lib/containers
          name: varlibcontainers
      env:
        - name: BEARER_TOKEN
          valueFrom:
            secretKeyRef:
              key: token
              name: $(params.BEARER_TOKEN_SECRET_NAME)
              optional: true
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: aws_access_key_id
              name: $(params.AWS_SECRET_NAME)
              optional: true
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: aws_secret_access_key
              name: $(params.AWS_SECRET_NAME)
              optional: true
      script: "#!/bin/bash\nset -euo pipefail\n\n# S3 transfer settings optimized
        for parallel multipart downloads\nMAX_CONCURRENT=50\nMULTIPART_THRESHOLD=\"64MB\"\nMULTIPART_CHUNKSIZE=\"16MB\"\n\n#
        Use NVMe-backed emptyDir for downloads (faster than workspace PVC)\nDOWNLOAD_DIR=\"/var/lib/containers/downloads\"\nmkdir
        -p \"$DOWNLOAD_DIR\"\n\n# Configure AWS CLI once (avoid race condition
        in parallel downloads)\nmkdir -p ~/.aws\nprintf '%s\\n' \\\n  \"[default]\"
        \\\n  \"s3 =\" \\\n  \"  max_concurrent_requests = ${MAX_CONCURRENT}\"
        \\\n  \"  multipart_threshold = ${MULTIPART_THRESHOLD}\" \\\n  \"
        \ multipart_chunksize = ${MULTIPART_CHUNKSIZE}\" \\\n  >~/.aws/config\n\ndownload()
        {\n  local url=\"$1\"\n  local file=\"$2\"\n\n  local directory\n
        \ directory=$(dirname \"${file}\")\n  echo \"Found directory of ${file}
        to be ${directory}\"\n  if [ ! -d \"${directory}\" ]; then\n    echo
        \"Found that directory ${directory} does not exist. Creating.\"\n
        \   mkdir -p \"${directory}\"\n  fi\n\n  if [ -n \"${AWS_ACCESS_KEY_ID:-}\"
        ] && [ -n \"${AWS_SECRET_ACCESS_KEY:-}\" ]; then\n    echo \"Found
        both aws credentials secret with both aws_access_key_id and aws_secret_access_key.
        Assuming S3 bucket\"\n\n    # Extract host and path from URL\n    local
        host path\n    host=$(echo -n \"$url\" | awk -F '/' '{print $3}')\n
        \   path=$(echo \"$url\" | cut -d/ -f4-)\n\n    echo \"Parsing S3
        URL - Host: ${host}, Path: ${path}\"\n\n    local endpoint_url=\"\"
        bucket=\"\" region=\"\" s3_uri=\"\" key=\"\"\n\n    if [[ \"$host\"
        == *.s3.*.amazonaws.com ]]; then\n      # AWS virtual-hosted-style:
        bucket.s3.region.amazonaws.com/key\n      bucket=$(echo \"$host\"
        | cut -d. -f1)\n      region=$(echo \"$host\" | cut -d. -f3)\n      s3_uri=\"s3://${bucket}/${path}\"\n
        \     echo \"Detected AWS virtual-hosted style URL\"\n    elif [[
        \"$host\" == s3.*.amazonaws.com ]]; then\n      # AWS path-style:
        s3.region.amazonaws.com/bucket/key\n      region=$(echo \"$host\"
        | cut -d. -f2)\n      bucket=$(echo \"$path\" | cut -d/ -f1)\n      key=$(echo
        \"$path\" | cut -d/ -f2-)\n      s3_uri=\"s3://${bucket}/${key}\"\n
        \     echo \"Detected AWS path-style URL\"\n    elif [[ \"$host\"
        == *.s3.*.cloud-object-storage.appdomain.cloud ]]; then\n      # IBM
        Cloud COS virtual-hosted: bucket.s3.region.cloud-object-storage.appdomain.cloud/key\n
        \     bucket=$(echo \"$host\" | cut -d. -f1)\n      region=$(echo
        \"$host\" | cut -d. -f3)\n      s3_uri=\"s3://${bucket}/${path}\"\n
        \     endpoint_url=\"https://s3.${region}.cloud-object-storage.appdomain.cloud\"\n
        \     echo \"Detected IBM Cloud COS virtual-hosted style URL\"\n    elif
        [[ \"$host\" == s3.*.cloud-object-storage.appdomain.cloud ]]; then\n
        \     # IBM Cloud COS path-style: s3.region.cloud-object-storage.appdomain.cloud/bucket/key\n
        \     region=$(echo \"$host\" | cut -d. -f2)\n      bucket=$(echo
        \"$path\" | cut -d/ -f1)\n      key=$(echo \"$path\" | cut -d/ -f2-)\n
        \     s3_uri=\"s3://${bucket}/${key}\"\n      endpoint_url=\"https://${host}\"\n
        \     echo \"Detected IBM Cloud COS path-style URL\"\n    elif [[
        \"$host\" == *s3* ]] || [[ \"$host\" == *minio* ]] || [[ \"$host\"
        == *storage* ]]; then\n      # Generic S3-compatible storage: host/bucket/key\n
        \     bucket=$(echo \"$path\" | cut -d/ -f1)\n      key=$(echo \"$path\"
        | cut -d/ -f2-)\n      s3_uri=\"s3://${bucket}/${key}\"\n      endpoint_url=\"https://${host}\"\n
        \     echo \"Detected generic S3-compatible storage URL\"\n    else\n
        \     # URL doesn't look like S3, fall back to curl\n      echo \"URL
        doesn't match known S3 patterns, falling back to curl\"\n      curl
        --fail --silent --show-error --location \"$url\" -o \"$file\"\n      return\n
        \   fi\n\n    echo \"S3 URI: ${s3_uri}, Bucket: ${bucket}, Region:
        ${region:-auto}\"\n    [ -n \"${endpoint_url}\" ] && echo \"Endpoint
        URL: ${endpoint_url}\"\n\n    if ! command -v aws &>/dev/null; then\n
        \     echo \"ERROR: AWS CLI is required for S3 downloads but was not
        found\"\n      exit 1\n    fi\n\n    echo \"Using AWS CLI for download
        with parallel multipart transfers\"\n    if [ -n \"${endpoint_url}\"
        ]; then\n      aws s3 cp --only-show-errors --endpoint-url \"${endpoint_url}\"
        \"${s3_uri}\" \"${file}\"\n    else\n      aws s3 cp --only-show-errors
        \"${s3_uri}\" \"${file}\"\n    fi\n  elif [ -n \"${BEARER_TOKEN:-}\"
        ]; then\n    echo \"Found bearer token. Using it for authentication.\"\n
        \   curl --fail --silent --show-error -H \"Authorization: Bearer ${BEARER_TOKEN}\"
        --location \"$url\" -o \"$file\"\n  else\n    curl --fail --silent
        --show-error --location \"$url\" -o \"$file\"\n  fi\n}\n\nset -u\n\necho
        \"Selecting auth for $IMAGE\"\nselect-oci-auth $IMAGE >auth.json\n\necho
        \"Extracting artifact_type\"\nARTIFACT_TYPE=$(cat \"$(pwd)/source/$OCI_COPY_FILE\"
        | yq '.artifact_type')\n\nREPO=${IMAGE%:*}\necho \"Found that ${REPO}
        is the repository for ${IMAGE}\"\n\ncat >artifact-manifest.json <<EOL\n{\n
        \ \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n
        \ \"artifactType\": \"${ARTIFACT_TYPE}\",\n  \"config\": {\n    \"mediaType\":
        \"application/vnd.oci.empty.v1+json\",\n    \"digest\": \"sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\",\n
        \   \"size\": 2,\n    \"data\": \"e30=\"\n  },\n  \"layers\": [],\n
        \ \"annotations\": {\n    \"org.opencontainers.image.created\": \"$(date
        -u +%Y-%m-%dT%H:%M:%SZ)\"\n  }\n}\nEOL\n\necho \"Ensuring that the
        empty blob exists, for the image manifest config.\"\nif ! echo -n
        \"{}\" | retry oras blob push \\\n  --registry-config auth.json \\\n
        \ ${REPO}@sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        \\\n  --media-type application/vnd.oci.empty.v1+json --size 2 -; then\n
        \ echo \"Failed to push empty blob to registry\"\n  exit 1\nfi\n\n#
        Validate PARALLEL_JOBS\nif ! [[ \"$PARALLEL_JOBS\" =~ ^[1-9][0-9]*$
        ]]; then\n  echo \"ERROR: PARALLEL_JOBS must be a positive integer,
        got: $PARALLEL_JOBS\"\n  exit 1\nfi\n\n# Collect all varfiles\nshopt
        -s nullglob\nALL_VARFILES=(\"/var/workdir\"/vars/*)\nshopt -u nullglob\nTOTAL=${#ALL_VARFILES[@]}\n\nif
        [ \"$TOTAL\" -eq 0 ]; then\n  echo \"No artifacts to process\"\nelse\n
        \ echo \"Processing $TOTAL files in batches of $PARALLEL_JOBS\"\nfi\n\nfor
        ((batch = 0; batch < TOTAL; batch += PARALLEL_JOBS)); do\n  BATCH_END=$((batch
        + PARALLEL_JOBS))\n  [ \"$BATCH_END\" -gt \"$TOTAL\" ] && BATCH_END=$TOTAL\n\n
        \ echo \"=== Batch $((batch / PARALLEL_JOBS + 1)): files $batch-$((BATCH_END
        - 1)) ===\"\n\n  # Check which files need processing (parallel existence
        checks)\n  declare -a TO_PROCESS=()\n  declare -a CHECK_RESULTS=()\n
        \ PIDS=()\n  for ((i = batch; i < BATCH_END; i++)); do\n    ( \n      #
        shellcheck source=/dev/null\n      source \"${ALL_VARFILES[$i]}\"\n
        \     if oras blob fetch --registry-config auth.json --descriptor
        \"${REPO}@sha256:${OCI_ARTIFACT_DIGEST}\" &>/dev/null; then\n        exit
        0 # exists\n      else\n        exit 1 # missing\n      fi\n    )
        &\n    PIDS+=(\"$!\")\n    CHECK_RESULTS+=(\"$i\")\n  done\n\n  for
        idx in \"${!PIDS[@]}\"; do\n    i=\"${CHECK_RESULTS[$idx]}\"\n    #
        shellcheck source=/dev/null\n    source \"${ALL_VARFILES[$i]}\"\n
        \   if wait \"${PIDS[$idx]}\"; then\n      echo \"Blob ${OCI_FILENAME}
        exists, skipping\"\n    else\n      TO_PROCESS+=(\"${ALL_VARFILES[$i]}\")\n
        \   fi\n  done\n\n  # Phase 1: Parallel downloads\n  if [ ${#TO_PROCESS[@]}
        -gt 0 ]; then\n    echo \"--- Downloading ${#TO_PROCESS[@]} files
        ---\"\n    declare -A PID_TO_FILE=()\n    for varfile in \"${TO_PROCESS[@]}\";
        do\n      ( \n        # shellcheck source=/dev/null\n        source
        \"$varfile\"\n        DLPATH=\"${DOWNLOAD_DIR}/${OCI_FILENAME}\"\n
        \       echo \"[DL] Downloading ${OCI_FILENAME}...\"\n        if !
        download \"$OCI_SOURCE\" \"$DLPATH\"; then\n          echo \"[DL]
        ERROR: Failed to download ${OCI_FILENAME}\"\n          exit 1\n        fi\n
        \       if ! echo \"$OCI_ARTIFACT_DIGEST  $DLPATH\" | sha256sum --check
        --quiet; then\n          echo \"[DL] ERROR: Checksum mismatch for
        ${OCI_FILENAME}\"\n          exit 1\n        fi\n        echo \"[DL]
        Downloaded ${OCI_FILENAME} [OK]\"\n      ) &\n      PID_TO_FILE[$!]=\"$varfile\"\n
        \   done\n    FAILED=0\n    for pid in \"${!PID_TO_FILE[@]}\"; do\n
        \     if ! wait \"$pid\"; then\n        # shellcheck source=/dev/null\n
        \       source \"${PID_TO_FILE[$pid]}\"\n        echo \"ERROR: Download
        failed for ${OCI_FILENAME}\"\n        FAILED=1\n      fi\n    done\n
        \   [ $FAILED -eq 1 ] && exit 1\n\n    # Phase 2: Parallel pushes\n
        \   echo \"--- Pushing ${#TO_PROCESS[@]} files ---\"\n    PID_TO_FILE=()\n
        \   for varfile in \"${TO_PROCESS[@]}\"; do\n      ( \n        # shellcheck
        source=/dev/null\n        source \"$varfile\"\n        DLPATH=\"${DOWNLOAD_DIR}/${OCI_FILENAME}\"\n
        \       echo \"[PUSH] Pushing ${OCI_FILENAME}...\"\n        if ! retry
        oras blob push --registry-config auth.json \"${REPO}\" --media-type
        \"${OCI_ARTIFACT_TYPE}\" \"$DLPATH\"; then\n          echo \"[PUSH]
        ERROR: Failed to push ${OCI_FILENAME}\"\n          exit 1\n        fi\n
        \       echo \"[PUSH] Pushed ${OCI_FILENAME} [OK]\"\n      ) &\n      PID_TO_FILE[$!]=\"$varfile\"\n
        \   done\n    FAILED=0\n    for pid in \"${!PID_TO_FILE[@]}\"; do\n
        \     if ! wait \"$pid\"; then\n        # shellcheck source=/dev/null\n
        \       source \"${PID_TO_FILE[$pid]}\"\n        echo \"ERROR: Push
        failed for ${OCI_FILENAME}\"\n        FAILED=1\n      fi\n    done\n
        \   [ $FAILED -eq 1 ] && exit 1\n\n    # Cleanup\n    for varfile
        in \"${TO_PROCESS[@]}\"; do\n      # shellcheck source=/dev/null\n
        \     source \"$varfile\"\n      rm -f \"${DOWNLOAD_DIR}/${OCI_FILENAME}\"\n
        \   done\n  fi\n\n  # Fetch descriptors in parallel\n  echo \"---
        Fetching descriptors ---\"\n  declare -A PID_TO_FILE=()\n  for ((i
        = batch; i < BATCH_END; i++)); do\n    ( \n      # shellcheck source=/dev/null\n
        \     source \"${ALL_VARFILES[$i]}\"\n      echo \"[DESC] Fetching
        descriptor for ${OCI_FILENAME}...\"\n      if ! retry oras blob fetch
        --registry-config auth.json --descriptor \"${REPO}@sha256:${OCI_ARTIFACT_DIGEST}\"
        >\"${DOWNLOAD_DIR}/desc_${i}.json\" 2>&1; then\n        echo \"[DESC]
        ERROR: Failed to fetch descriptor for ${OCI_FILENAME}\"\n        cat
        \"${DOWNLOAD_DIR}/desc_${i}.json\" 2>/dev/null || true\n        exit
        1\n      fi\n      OCI_ARTIFACT_TYPE=\"$OCI_ARTIFACT_TYPE\" yq -oj
        -i '.mediaType = env(OCI_ARTIFACT_TYPE)' \"${DOWNLOAD_DIR}/desc_${i}.json\"\n
        \     OCI_FILENAME=\"$OCI_FILENAME\" yq -oj -i '.annotations.\"org.opencontainers.image.title\"
        = env(OCI_FILENAME)' \"${DOWNLOAD_DIR}/desc_${i}.json\"\n      echo
        \"[DESC] Fetched descriptor for ${OCI_FILENAME} [OK]\"\n    ) &\n
        \   PID_TO_FILE[$!]=\"${ALL_VARFILES[$i]}\"\n  done\n  FAILED=0\n
        \ for pid in \"${!PID_TO_FILE[@]}\"; do\n    if ! wait \"$pid\"; then\n
        \     # shellcheck source=/dev/null\n      source \"${PID_TO_FILE[$pid]}\"\n
        \     echo \"ERROR: Descriptor fetch failed for ${OCI_FILENAME}\"\n
        \     FAILED=1\n    fi\n  done\n  [ $FAILED -eq 1 ] && exit 1\n\n
        \ # Append to manifest (sequential - file writes not parallelizable)\n
        \ for ((i = batch; i < BATCH_END; i++)); do\n    yq -oj -i \".layers
        += $(cat \"${DOWNLOAD_DIR}/desc_${i}.json\")\" artifact-manifest.json\n
        \   rm -f \"${DOWNLOAD_DIR}/desc_${i}.json\"\n  done\n\n  unset TO_PROCESS
        PIDS PID_TO_FILE CHECK_RESULTS FAILED\ndone\n\necho \"Pushing complete
        artifact manifest to ${IMAGE}\"\nif ! retry oras manifest push --registry-config
        auth.json \"${IMAGE}\" artifact-manifest.json; then\n  echo \"Failed
        to push complete artifact manifest to ${IMAGE}\"\n  exit 1\nfi\n\nif
        ! RESULTING_DIGEST=$(retry oras resolve --registry-config auth.json
        \"${IMAGE}\"); then\n  echo \"Failed to get digest for ${IMAGE} from
        registry\"\n  exit 1\nfi\necho -n \"$RESULTING_DIGEST\" | tee \"$(results.IMAGE_DIGEST.path)\"\necho
        -n \"$IMAGE\" | tee \"$(results.IMAGE_URL.path)\"\necho -n \"${IMAGE}@${RESULTING_DIGEST}\"
        >\"$(results.IMAGE_REF.path)\"\n"
      computeResources:
        limits:
          memory: 4Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        capabilities:
          add:
            - SETFCAP
    - name: sbom-generate
      image: quay.io/konflux-ci/mobster:1.1.0-1768294847@sha256:ff738d9032c9885731bc96383e720ced2469649fa7148b76b58b7492c9bb82b8
      workingDir: /var/workdir
      script: |
        #!/bin/bash
        set -euo pipefail

        IMAGE_URL=$(cat "$(results.IMAGE_URL.path)")
        IMAGE_DIGEST=$(cat "$(results.IMAGE_DIGEST.path)")
        OCI_COPY_FILE_PATH="$(pwd)/source/$OCI_COPY_FILE"

        mobster generate \
          --output sbom.json \
          oci-artifact \
          --oci-copy-yaml "$OCI_COPY_FILE_PATH" \
          --image-pullspec "$IMAGE_URL" \
          --image-digest "$IMAGE_DIGEST" \
          --sbom-type "$SBOM_TYPE"
    - name: upload-sbom
      image: quay.io/konflux-ci/appstudio-utils:1610c1fc4cfc9c9053dbefc1146904a4df6659ef@sha256:90ac97b811073cb99a23232c15a08082b586c702b85da6200cf54ef505e3c50c
      workingDir: /var/workdir
      script: |
        # Pre-select the correct credentials to work around cosign not supporting the containers-auth.json spec
        mkdir -p /tmp/auth && select-oci-auth "$(cat "$(results.IMAGE_REF.path)")" >/tmp/auth/config.json
        DOCKER_CONFIG=/tmp/auth cosign attach sbom --sbom sbom.json --type "$SBOM_TYPE" "$(cat "$(results.IMAGE_REF.path)")"
    - name: report-sbom-url
      image: quay.io/konflux-ci/yq:latest@sha256:eb5b7311cff72c42f1d816fcf4f565829f58bbbd4c51503ed390b8f4e7a3fe89
      workingDir: /var/workdir
      script: |
        #!/bin/bash
        REPO=${IMAGE%:*}
        echo "Found that ${REPO} is the repository for ${IMAGE}"
        SBOM_DIGEST=$(sha256sum sbom.json | awk '{ print $1 }')
        echo "Found that ${SBOM_DIGEST} is the SBOM digest"
        echo -n "${REPO}@sha256:${SBOM_DIGEST}" | tee $(results.SBOM_BLOB_URL.path)
